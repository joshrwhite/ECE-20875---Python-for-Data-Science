\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}

\usepackage{comment}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{hyperref}
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black
}

\setlength{\parindent}{0pt}
\parskip 1.5ex

% Alias for right/left environment
\renewcommand\r{\right}
\renewcommand\l{\left}

% Matrices and Vectors
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bzero}{\mathbf{0}}

% Norms
\providecommand{\norm}[1]{\l\lVert#1\r\rVert}
\providecommand{\normo}[1]{\l\lVert#1\r\rVert_1}
\providecommand{\normz}[1]{\l\lVert#1\r\rVert_0}
\providecommand{\normf}[1]{\l\lVert#1\r\rVert_F}

% Absolute value
\providecommand{\abs}[1]{\l|#1\r|}

% Probability
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}
\providecommand{\Prob}[1]{P\l(#1\r)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\begin{document}
\title{\textbf{Homework 7: Linear Regression}}
\date{ }
\maketitle

This homework covers several regression topics, and will give you practice with the {\tt numpy} and {\tt sklearn} libraries in Python. It has both a coding and a writeup component.

\section{Goals}
In this homework you will:
\begin{enumerate}
\item Build linear regression models to serve as predictors from input data
\item Parse input data into feature matrices and target variables
\item Use cross validation to find the best regularization parameter for a dataset
\end{enumerate}

\section{Background}
Before attempting the homework, please review the notes on linear regression. In addition to what is covered there, the following background may be useful:

\subsection{CSV Processing in Python}
Like .txt, .csv (comma-separated values) is a useful file format for storing data. In a CSV file, each line is a data record, and different fields of the record are separated by commas, making them two-dimensional data tables (i.e., records by fields). Typically, the first row and first column are headings for the fields and records.

Python's {\tt pandas} module helps manage two-dimensional data tables. We can read a CSV as follows:
\begin{verbatim}
import pandas as pd
data = pd.read_csv(`data.csv')
\end{verbatim}
To see a small snippet of the data, including the headers, we can write {\tt data.head()}. Once we know which columns we want to use as features (say `A', `B', `D') and which to use as a target variable (say `C`), we can build our feature matrix and target vector by referencing the header:
\begin{verbatim}
X = data[[`A', `B', `D']]
y = data[[`C']]
\end{verbatim}
More details on Pandas can be found here: \url{https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html}.

\subsection{Matrix Algebra in Python}
Python offers computationally efficient functions for linear algebra operations through the {\tt numpy} library. Suppose {\tt A} is a list of $m$ lists, each having $n$ numerical items. Numpy will treat {\tt A} as an $m \times n$ matrix. If we want to transpose {\tt A}, we can write:
\begin{verbatim}
import numpy as np
AT = np.transpose(A)
\end{verbatim}
if {\tt B} is another $m \times n$ matrix, we can perform the matrix operation $A^T B$ by writing:
\begin{verbatim}
AB = np.dot(np.transpose(A),B)
\end{verbatim}
Note that if $n = 1$, i.e., {\tt A} and {\tt B} are both vectors with $m$ elements, this operation takes the dot product between the vectors.

If {\tt A} is a square $n \times n$ matrix, we can find its inverse (if it exists) with the following:
\begin{verbatim}
Ainv = np.linalg.inv(A)
\end{verbatim}
Other useful matrix operations can be found here: \url{https://docs.scipy.org/doc/numpy/reference/routines.linalg.html}.

\subsection{Linear Regression in Python}
Python offers several standard machine learning models with optimized implementations in the {\tt sklearn} library. Suppose we have a feature matrix {\tt X} and a target variable vector {\tt y}. To train a standard linear regression model, we can write:
\begin{verbatim}
from sklearn.linear_model import LinearRegression
model_lin = linear_model.LinearRegression(fit_intercept = True)
model_lin.fit(X, y)
\end{verbatim}
Then, if we have a feature matrix {\tt Xn} of new samples, we can predict the target variables (if we know the model is performing well) by applying the trained model:
\begin{verbatim}
yn = model_lin.predict(Xn)
\end{verbatim}
And we can view the parameters of the model by writing:
\begin{verbatim}
model_lin.get_params()
\end{verbatim}
There are also a few different versions of regularized linear regression models in {\tt sklearn}. One of the most common is ridge regression, which has a single regularization parameter $\lambda$. To train with $\lambda = 0.2$, for instance, we can write:
\begin{verbatim}
from sklearn.linear_model import Ridge
model_ridge = linear_model.Ridge(alpha = 0.2, fit_intercept = True)
model_ridge.fit(X, y)
\end{verbatim}
More regression models in Python can be found here: \url{https://scikit-learn.org/stable/supervised_learning.html#supervised-learning}.

\section{Instructions}
\renewcommand{\thesubsection}{\thesection.\number\numexpr\value{subsection}-1\relax}

\subsection{Setting up your repository}
Click the link on Piazza to set up your repository for HW 7, then clone it.

The repository should contain two files aside from this readme, both of which you will use in Problem 1:

\begin{enumerate}
\item {\tt plotfit.py}, a starter file with functions, instructions, and a skeleton that you will fill out in Problem 1.
\item {\tt poly.txt}, a data file for Problem 1 where each row is a datapoint in the format: {\tt x y}, with {\tt x} being the explanatory and ${\tt y}$ being the target variable.
\item {\tt regularize-cv.py}, a starter file with functions, instructions, and a skeleton that you will fill out in Problem 2.
\item {\tt diamonds.csv}, a data file for Problem 2 where each row has 10 attributes corresponding to a diamond.
\end{enumerate}



\subsection{Problem 1: Polynomial regression}
A common misconception is that linear regression can only be used to fit a linear relationship. We can fit more complicated functions of the explanatory variables by defining new features that are functions of the existing features. A common class of models is the \textbf{polynomial}, with an $n$th degree polynomial being of the form
$$\hat{y}_n(x) = c_n x^n + c_{n-1} x^{n-1} + \cdots + c_1 x + c_0$$
with the $n+1$ parameters $c_n, ..., c_0$. So $n=1$ corresponds to a line, $n=2$ to a quadratic, $n=3$ to a cubic, and so forth.

In this problem, you will build a series of functions that fit polynomials of different degrees to a dataset. You will then use this to determine the best fit to a dataset by comparing the models from different degrees visually against a scatterplot of the data, and make a prediction for an unseen sample. More specifically:
\begin{enumerate}
\item Complete the functions in {\tt polyfit.py}, which accepts as input a dataset to be fit and polynomial degrees to be tried, and outputs a list of fitted models. The specifications for the {\tt main}, {\tt feature\_matrix}, and {\tt least\_squares} functions are contained as comments in the skeleton code. The key steps are parsing the input data, creating the feature matrix, and solving the least squares equations.

\item Use your completed {\tt polyfit.py} to find fitted polynomial coefficients for $n = 1, 2, 3, 4, 5$ on the {\tt data1.txt} dataset. Write out the resulting estimated functions $\hat{y}_n(x)$ for each $n$.

\item Use the {\tt scatter} and {\tt plot} functions in the {\tt matplotlib.pyplot} module to visualize the dataset and these fitted models on a single graph (i.e., for each $x$, plot $y$, $\hat{y}_1(x)$, ..., $\hat{y}_5(x)$). Be sure to vary colors and include a legend so that each curve can be distinguished. What degree polynomial does the relationship seem to follow? Explain.

\item If we measured a new datapoint $x = 2$, what would be the predicted value of $y$?
\end{enumerate}
Note that in this problem, \textbf{you are not permitted to use the {\tt sklearn} library}. You must use matrix operations in {\tt numpy} to solve the least squares equations.

Once you have completed {\tt polyfit.py}, if you run the test case provided, it should output (rounded to 6 significant digits)
$${\tt [ [6.60741, 11.2584, -231.022], [ -0.001170, 0.829407, 0.242220, -0.166368, -173.823] ] }$$

\subsection{Problem 2: Regularized regression}
Regularization techniques like ridge regression introduce an extra model parameter, namely, the regularization parameter $\lambda$. To determine the best value of $\lambda$ for a given dataset, we often employ cross validation, where we compare the error of the trained model with different values of $\lambda$ on a test set, and choose the one yielding lowest error.

In this problem, you will complete the starter code in {\tt regularize-cv.py} that employs cross validation in selecting the best combination of model parameters $\beta$ and regularization parameter $\lambda$ for a predictor on a given dataset. We use the {\tt diamonds.csv} dataset ({\tt http://vincentarelbundock.github.io/Rdatasets/datasets.html}) here, which contains the prices and nine descriptive attributes (carats, cut, color, clarity, depth, table, x, y, z) of roughly 54,000 diamonds. From the input data, you will train a ridge regression model on these nine attributes for different values of $\lambda$, find the best, and use the result to predict the price of a new diamond given a set of input features describing it. More specifically:
\begin{enumerate}
\item Complete the function {\tt normalize} to normalize the feature matrix $\mathbf{X}$. The nine columns of this matrix, $\mathbf{X} = [\mathbf{x}_1 \; \mathbf{x}_2 \; \cdots \; \mathbf{x}_9]$, must each be normalized to have a mean of $0$ and a standard deviation of $1$.

\item Define the range of $\lambda$ to test in {\tt main} as $0.1, 0.2, ..., 1.0, 1.5, ..., 10.0, 11.0, ..., 100$. This type of logarithmic scale, moving from a smaller to a larger increment, is common for regularization.

\item Complete the function {\tt train\_model} to fit a ridge regression model with regularization parameter $\lambda = l$ on a training dataset $\mathbf{X}_{train}$, $\mathbf{y}_{train}$. You may use the {\tt linear\_model.Ridge} class in {\tt sklearn} to do this. Note that the partition of the training and testing set has already been done for you.

\item Complete the function {\tt error} to calculate the mean squared error of the model on a testing dataset $\mathbf{X}_{test}$, $\mathbf{y}_{test}$. 

\item Complete the code in {\tt main} for plotting the mean squared error as a function of $\lambda$, and for finding the {\tt model} and {\tt mse} corresponding to the best {\tt lmbda}. Be sure to include a title and axes labels with your plot.

\item Using the coefficients $\beta = (c_9, \;c_8, \; \cdots \; c_0)$ from the returned {\tt model\_best}, write out the equation $\hat{y}(\mathbf{x}) = c_9 x_1 + c_8 x_2 + \cdots + c_1 x_9 + c_0$ of your fitted model for a sample $\mathbf{x}$. What is the predicted price for a $0.25$ carat, $3$ cut, $3$ color, $5$ clarity, $60$ depth, $55$ table, $4$ x, $3$ y, $2$ z diamond?
\end{enumerate}
Once you have completed {\tt regularize\-cv.py}, if you set ${\tt lmbda} = [1, 100]$, your output message should be `Best lambda tested is 1, which yields an MSE of $1812351.1908771885$'.


\section{What to Submit}
For each problem, you must submit (i) your completed version of the starter code, and (ii) a writeup as a separate PDF document (e.g., {\tt problem1\_writeup.pdf} and {\tt problem2\_writeup.pdf}).


\section{Submitting your Code}
Please tag the version of the code that you want to submit with submission, as you did in the previous HW.

Don't forget to commit the code that you want to submit before tagging your submission. You have to do this in two steps.



\end{document}